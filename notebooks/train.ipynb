{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6068 sequences.\n",
      "# non-empty sequences: 5043\n",
      "Minimum length: 2\n",
      "# sequences with enough length: 4599\n",
      "====== examples ======\n",
      "0 上不乍二伐伐兄光兵出吉君君坪居居居左己𠭁𠭁𠭁𠭁旬旬星是是東畜相箸胃自雨雨首…………………\n",
      "1 一三不乙二亥亥以以以以八出出利利利可可壬女女女子居左𠭁必日旬木欠死甬甲甲發白色行視軍量黃………………\n",
      "2 其二在長…\n",
      "3 …重鎰…足…重八鎰…鎰一銖\n",
      "4 十月乙丑\n",
      "5 之上與𫺕哲王之威俈…尹郘逯㠯王命賜舒方御歲愲\n",
      "6 廷等\n",
      "7 二褥席\n",
      "8 無及也已入之或入之至之或至之\n",
      "9 咎告尒某邑之社…又石曾孫某邑不幸命…敢用五器宮之以石…邑是…昌大縵…君夫君婦一\n",
      "10 君高石奴君之神霝攸政民人句史四方之羣明歸曾孫某之邑者亓麥…亓麥徇見某乃𢝫…\n",
      "11 …諥與悆奴隧重者女奴見亓父奴見亓母女見亓妻奴見亓子奴百湩川之歸…奴…內三\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "\n",
    "# Load texts\n",
    "from utils import load_json\n",
    "from data.utils import parse_label\n",
    "\n",
    "\n",
    "def load_texts(min_len: int = 2) -> list:\n",
    "    TEXTS_PATH = '../data/sequences/seq_texts.json'\n",
    "    texts = load_json(TEXTS_PATH)\n",
    "    texts = [seq['text'] for seq in texts]\n",
    "    print(f'Loaded {len(texts)} sequences.')\n",
    "    texts = [t for t in texts if len(t) > 0]  # Many sequences are empty\n",
    "    print(f'# non-empty sequences: {len(texts)}')\n",
    "    texts = [t for t in texts if len(t) >= min_len]\n",
    "    print(f'Minimum length: {min_len}')\n",
    "    print(f'# sequences with enough length: {len(texts)}')\n",
    "    texts = [\n",
    "        ''.join(\n",
    "            [parse_label(c, True, comb_token='…', unk_token='…') for c in seq]\n",
    "        ) for seq in texts\n",
    "    ]\n",
    "    return texts\n",
    "\n",
    "texts = load_texts()\n",
    "print('====== examples ======')\n",
    "for i in range(12):\n",
    "    if len(texts[i]) > 0:\n",
    "        print(i, texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(26419, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"KoichiYasuoka/roberta-classical-chinese-base-char\"\n",
    "TOKENIZER_PATH = 'tokenization/tokenizer'\n",
    "print('Loading tokenizer and model...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into [3679, 4139]...\n",
      "Train size: 3679\n",
      "Dev size: 460\n",
      "Test size: 460\n",
      "Building dataset...\n"
     ]
    }
   ],
   "source": [
    "# Split data by 8:1:1\n",
    "import random\n",
    "\n",
    "from typing import Tuple, List\n",
    "from dataset import ChujianMLMDataset\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    texts: List[str]\n",
    ") -> Tuple[ChujianMLMDataset, ChujianMLMDataset, ChujianMLMDataset]:\n",
    "    random.seed(0)\n",
    "    split_idx = [int(len(texts) * 0.8), int(len(texts) * 0.9)]\n",
    "    print(f'Splitting data into {split_idx}...')\n",
    "    print(f'Train size: {split_idx[0]}')\n",
    "    print(f'Dev size: {split_idx[1] - split_idx[0]}')\n",
    "    print(f'Test size: {len(texts) - split_idx[1]}')\n",
    "    random.shuffle(texts)\n",
    "    train_texts = texts[:int(split_idx[0])]\n",
    "    dev_texts = texts[int(split_idx[0]):int(split_idx[1])]\n",
    "    test_texts = texts[int(split_idx[1]):]\n",
    "    print('Building dataset...')\n",
    "    train_data = ChujianMLMDataset(train_texts, tokenizer)\n",
    "    dev_data = ChujianMLMDataset(dev_texts, tokenizer)\n",
    "    test_data = ChujianMLMDataset(test_texts, tokenizer)\n",
    "    return train_data, dev_data, test_data\n",
    "\n",
    "train_data, dev_data, test_data = get_dataset(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Training ======\n",
      "  Num steps: 58\n",
      "  Num examples: 3679\n",
      "  Num epochs: 2\n",
      "  Batch size: 64\n",
      "  Log interval: 10\n",
      "Start epoch 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 20\u001b[0m\n\u001b[0;32m      8\u001b[0m output_dir \u001b[39m=\u001b[39m Path(\n\u001b[0;32m      9\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mresult/roberta-classical-chinese-base-char\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     10\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m{\u001b[39;00mlr\u001b[39m}\u001b[39;00m\u001b[39m-bs\u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     14\u001b[0m     model,\n\u001b[0;32m     15\u001b[0m     output_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     log_interval\u001b[39m=\u001b[39mlog_interval\n\u001b[0;32m     19\u001b[0m )\n\u001b[1;32m---> 20\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(train_data, dev_data)\n",
      "File \u001b[1;32me:\\donny\\code\\school\\research\\chujian\\chujian-bert\\trainer.py:153\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, train_data, dev_data, do_resume)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m  Log interval: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_interval\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcur_ep \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_epochs:\n\u001b[1;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_loader)\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_and_validate(dev_data)\n\u001b[0;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcur_ep \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32me:\\donny\\code\\school\\research\\chujian\\chujian-bert\\trainer.py:70\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStart epoch \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcur_ep\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step(batch)\n\u001b[0;32m     71\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mEpoch done\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\donny\\code\\school\\research\\chujian\\chujian-bert\\trainer.py:76\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(\u001b[39mself\u001b[39m, batch: \u001b[39mtuple\u001b[39m):\n\u001b[0;32m     75\u001b[0m     inputs, labels \u001b[39m=\u001b[39m batch\n\u001b[1;32m---> 76\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     77\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     78\u001b[0m     \u001b[39m# Forward pass\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "from trainer import Trainer\n",
    "from pathlib import Path\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "num_epochs = 2\n",
    "lr = 2e-5\n",
    "batch_size = 64\n",
    "log_interval = 10\n",
    "output_dir = Path(\n",
    "    'result/roberta-classical-chinese-base-char', \n",
    "    f'lr{lr}-bs{batch_size}',\n",
    ")\n",
    "\n",
    "train_collate_fn = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    output_dir,\n",
    "    train_collate_fn=train_collate_fn,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    log_interval=log_interval\n",
    ")\n",
    "trainer.train(train_data, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
