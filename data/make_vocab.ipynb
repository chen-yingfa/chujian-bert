{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['上', '不', '乍', '二', '伐', '伐', '兄', '光', '兵', '出', '吉', '君', '君', '坪', '居', '居', '居', '左', '己', '𠭁', '𠭁', '𠭁', '𠭁', '旬', '旬', '星', '是', '是', '東', '畜', '相', '箸', '胃', '自', '雨', '雨', '首', '{文一}', '{文一}', '{文一}', '{文一}', '{車丙}', '[UNK]', '[UNK]'], ['一', '三', '不', '乙', '二', '亥', '亥', '以', '以', '以', '以', '八', '出', '出', '利', '利', '利', '可', '可', '壬', '女', '女', '女', '子', '居', '左', '𠭁', '必', '日', '旬', '木', '欠', '死', '甬', '甲', '甲', '發', '白', '色', '行', '視', '軍', '量', '黃', '{一戈月}', '{井田土}', '[UNK]', '[UNK]', '[UNK]', '[UNK]'], ['其', '二', '在', '長', '{尾少}'], ['絇'], ['[UNK]'], ['[UNK]', '重', '鎰', '[UNK]', '足', '[UNK]', '重', '八', '鎰', '[UNK]', '鎰', '一', '銖'], ['十', '月', '乙', '丑'], [], ['之', '上', '與', '𫺕', '哲', '王', '之', '威', '俈', '{辶卜}', '尹', '郘', '逯', '㠯', '王', '命', '賜', '舒', '方', '御', '歲', '愲'], ['[UNK]']]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from utils import parse_label\n",
    "\n",
    "def dump_json(data, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "seq_file = (\n",
    "    'E:/donny/code/school/research/chujian/data/sequences/seq_texts.json')\n",
    "seqs = json.load(open(seq_file, 'r', encoding='utf-8'))\n",
    "seqs = [seq['text'] for seq in seqs]\n",
    "seqs = [[parse_label(c, use_comb_token=False) for c in seq] for seq in seqs]\n",
    "print(seqs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick words to add to vocabulary\n",
    "\n",
    "For words that are not in the original vocabulary, if it appear less than $k=10$ times, map it to \"？\" (fullwidth, because it's more frequent in the pretraining data), else, add it to the vocabulary. The \"？\" is already in the vocabulary, so we only add words that appear more than $k$ times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "model_name = \"KoichiYasuoka/roberta-classical-chinese-base-char\"\n",
    "# model_name = \"ethanyt/guwenbert-base\"\n",
    "cache_dir = \"E:/.cache/huggingface\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of new tokens: 101\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def iter_seqs(seqs: List[List[str]]):\n",
    "    for seq in seqs:\n",
    "        for c in seq:\n",
    "            yield c\n",
    "\n",
    "word_cnt = defaultdict(int)\n",
    "for c in iter_seqs(seqs):\n",
    "    word_cnt[c] += 1\n",
    "\n",
    "k = 10\n",
    "orig_vocab = tokenizer.vocab\n",
    "new_tokens = {'[COMB]'}\n",
    "for c in iter_seqs(seqs):\n",
    "    if c not in orig_vocab and word_cnt[c] >= k:\n",
    "        new_tokens.add(c)\n",
    "new_tokens = list(new_tokens)\n",
    "print(f\"Num of new tokens: {len(new_tokens)}\")\n",
    "dump_json(new_tokens, 'new_vocab.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenizer to ../tokenization/tokenizer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../tokenization/tokenizer\\\\tokenizer_config.json',\n",
       " '../tokenization/tokenizer\\\\special_tokens_map.json',\n",
       " '../tokenization/tokenizer\\\\vocab.txt',\n",
       " '../tokenization/tokenizer\\\\added_tokens.json',\n",
       " '../tokenization/tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the vocab of tokenizer\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "print('Saving tokenizer to ../tokenization/tokenizer')\n",
    "tokenizer.save_pretrained('../tokenization/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['綊', '墿', '？', '箄', '{ 竹  巫  口 }', '弩', '弓', '[comb]', ' 𡄹 ', '{ 弓  口  二 }', '？', '？', '[comb]']\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer\n",
    "text = \"綊墿？箄{竹巫口}弩弓[COMB]𡄹{弓口二}？？[COMB]\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
